# Curso de Scrapy
Scrapy es una de las herramientas de web scraping m谩s poderosas, utilizando python podremos generar un script capaz de hacer scraping hacia los gobiernos 

## Instalaci贸n
- Creamos una carpeta ra铆z del proyecto
- Inicializamos el repositorio con `git init` en git bash
- Generamos un virtual environment con `python -m venv venv`
- Generamos el archivo `.gitignore` y agregamos al directorio **venv/**
- Una vez terminada la generaci贸n de nuestro entorno lo inicializamos con `venv/scripts/activate`
- Instalamos los requerimientos del proyecto con `pip install autopep8 scrapy`
- **Opcional:** Generamos requirements.txt con `pip freeze > requirements.txt`
- Hacemos nuestro commit inicial con `git add .` y `git commit -m "message"`

## El framework as铆ncrono
Scrapy nos permite hacer **webscraping** y **webcrawiling**
Scrapy es un framework as铆ncrono, lo que nos permite generar requests de forma independiente de las anteriores
- Herramientas
-- Procesador Xpath interno
-- Interactive Shell interna
-- Exportaci贸n m煤ltiple [JSON, CSV, etc]
-- Seguimiento de reglas robots.txt autom谩tica 

## Comenzando

Una vez abierta la carpeta ra铆z del proyecto en VSCode con `code .` inicializamos un proyecto de Scrapy
- El c贸digo para ello es: `scrapy startproject <project_name>`
- Luego de esto nos pedir谩 cambiar de directorio a `cd <project_name>`
- Se generar谩 una nueva carpeta, en la cual vamos a hacer un nuevo archivo **.py** dentro de `<project_name>/<project_name>/file.py`

Dentro de nuestro archivo haremos la importaci贸n de la librer铆a con `import scrapy`, l贸gicamente no hablaremos de la construcci贸n del archivo pero si sobre algunas especificaciones, Scrapy es orientado a objetos por lo que nuestro proyecto se generar谩 basado en este paradigma.

La ejecuci贸n de un archivo se da a trav茅s de `scrapy crawl ClassName>name` (Refiri茅ndose a que el nombre a utilizar es el que est谩 dentro de la clase y dentro de la variable name)

## Generadores e Iteradores
*Recordamos el uso de estos.*
- *Un Iterador* dentro de Python es un elemento que pueda ser iterado, tales como los string, list, etc. Python posee una forma sencilla de iterar entre estos elementos, el ciclo **for**, pero este no es m谩s que _suggar syntax_ para facilitar la tarea original, el proceso ser铆a el siguiente:
-- Python toma el elemento y lo convierte en un objeto iterable
-- Utiliza la funci贸n built-in `next()` para poder seleccionar el elemento inicial o siguiente de acuerdo a las peticiones del usuario.
-- Al llegar al final de la iteraci贸n arroja un error de tipo `StopIteration`
- Un Generador es una forma pr谩ctica que tiene Python de retornar un elemento en una funci贸n sin tener que terminar con esta, esto se logra al reemplazar el uso de **return** por **yield** y utilizando next() para la el paso al siguiente iterable. Python llega hasta la declaraci贸n `yield` y deja marcado el estado de la misma, una vez empezado nuevamente un ciclo y a trav茅s de next(), `yield` se encargar谩 de devolver el valor correspondiente a la siguiente iteraci贸n, cosa que no ocurrir铆a dentro de `return`, ya que este se toma este como punto final de la funci贸n.

## Scrapy shell
隆Nos va a permitir hacer consultas de XPath de manera mejorada y eficaz!
A trav茅s de esta shell interactiva podemos hacer request de p谩ginas para obtener todo tipo de informaci贸n 煤til a trav茅s de:
- `scrapy shell 'url'` para hacer la llamada inicial

Y luego, abierta la shell, m煤ltiples funciones como:

-- `response.xpath('xpath-path').get() -/- .getall()` para obtener uno o m煤ltiples elementos xpath
-- `request.encoding` para obtener el encoding dado por la request
-- `request.status`, para saber el estado de la llamada
-- `request.headers` para obtener los headers del sitio y su informaci贸n
-- `request.body` para obtener el cuerpo de la llamada, osea el html

## Estructura de carpetas
Luego de creado nuestro directorio a trav茅s de `scrapy startproject <project_name>` nos encontraremos con varios archivos dentro de los directorios del proyecto, tales como:
- scrapy.cfg: Settings
- pipelines.py: Modificar desde la entrada en Spiders hasta su salida
- middlewares.py: Controlador de eventos durante la ejecuci贸n de la request
- settings.py: Archivo de configuraciones, tales como tiempos, nombres, cookies, headers y otras funciones

## Spiders 
*Una clase de python en la que decidimos qu茅 informaci贸n queremos, cu谩l no y c贸mo guardarla*

`parse()`: Analiza un archivo (response) para extraer informaci贸n valiosa del mismo

## Guardado como archivo
Para guardar un archivo con Scrapy se debe fijar el output desde la consola (Al menos por ahora) utilizando el flag -o y el nombre del output:
`scrapy crawl <file> -o <file_output>`